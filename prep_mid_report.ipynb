{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from huggingface\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset_test = dataset[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_significance(df_train, metric1, metric2):\n",
    "    correlation, p_value = scipy.stats.spearmanr(df_train[metric1], df_train[metric2])\n",
    "    print(f'Correlation between {metric1} and {metric2}')\n",
    "    print(f\"Correlation coefficient: {correlation}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    significant = False\n",
    "    if p_value < 0.05:\n",
    "        significant = True \n",
    "    return {\n",
    "                    \"metric1\": metric1,\n",
    "                    \"metric2\": metric2,\n",
    "                    \"correlation\": correlation, \n",
    "                    \"p-value\": p_value, \n",
    "                    \"signficant\": significant\n",
    "                }\n",
    "\n",
    "def plot(df_train, column1, column2, prefix='Raw'):\n",
    "    plt.figure(figsize=(8, 6))  \n",
    "    # plt.scatter(df_train[column1], df_train[column2], color='blue', alpha=0.6) \n",
    "    sns.regplot(x = df_train[column1], y = df_train[column2], color='green', scatter_kws={'alpha':0.5})\n",
    "    plt.title(f'Scatter Plot of {column1} vs {column2} on {prefix}')  \n",
    "    plt.xlabel(f'{column1}')  \n",
    "    plt.ylabel(f'{column2}')  \n",
    "    plt.grid(True) \n",
    "    plt.savefig(f'./{column1}_{column2}_{prefix}_correlation.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to json\n",
    "data = []\n",
    "for i in range(len(dataset_test)):\n",
    "    data.append({\"text\": dataset_test[i][\"text\"], \"gt_label\": dataset_test[i][\"label\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = {x['text']: x for x in data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append your baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_predicted_labels = \"/home/anmola/assignments_hw/sentiment-cartography/v2_dir_512_large/inference_results.json\"\n",
    "path_logit_scores = \"/home/anmola/assignments_hw/sentiment-cartography/single_v2_dir_512_large/inference_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_predicted_labels, \"r\") as f:\n",
    "    predicted_df = json.load(f)\n",
    "with open(path_logit_scores, \"r\") as f:\n",
    "    logit_df = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matched_pairs(s1, arr_s):\n",
    "    # find indexes of all elements in arr_s that s1 is a substring of\n",
    "    matched_pairs = []\n",
    "    for i, s in enumerate(arr_s):\n",
    "        if s1 in s:\n",
    "            matched_pairs.append(i)\n",
    "    assert(len(matched_pairs) > 0)\n",
    "    assert(len(matched_pairs)==1)\n",
    "    return matched_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:12<00:00, 3923.89it/s]\n"
     ]
    }
   ],
   "source": [
    "all_keys = list(data_df.keys())\n",
    "for elem in tqdm.tqdm(predicted_df):\n",
    "    curr_review_text = elem[\"review\"]\n",
    "    if curr_review_text in data_df:\n",
    "        matched_review_text = curr_review_text\n",
    "    else:\n",
    "        matched_key_idx = find_matched_pairs(curr_review_text, all_keys)\n",
    "        matched_review_text = all_keys[matched_key_idx[0]]\n",
    "    assert(\"multi_class_predicted_label\" not in data_df[matched_review_text])\n",
    "    data_df[matched_review_text][\"multiclass_predicted_label\"] = elem[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:13<00:00, 3803.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for elem in tqdm.tqdm(logit_df):\n",
    "    curr_review_text = elem[\"review\"]\n",
    "    if curr_review_text in data_df:\n",
    "        matched_review_text = curr_review_text\n",
    "    else:\n",
    "        matched_key_idx = find_matched_pairs(curr_review_text, all_keys)\n",
    "        matched_review_text = all_keys[matched_key_idx[0]]\n",
    "    assert(\"single_class_logit_score\" not in data_df[matched_review_text])\n",
    "    logits = elem[\"logit_scores\"]\n",
    "    softmax_scores = scipy.special.softmax(logits)\n",
    "    assert(len(softmax_scores) == 2)\n",
    "    data_df[matched_review_text][\"single_class_logit_score\"] = softmax_scores[1]\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Diddee's api scores\n",
    "api_scores_path = \"/home/anmola/assignments_hw/sentiment-cartography/data/didee_google_results.jsonl\"\n",
    "api_scores_data = []\n",
    "with open(api_scores_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        api_scores_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\",\n",
       " 'score': 0.6520000100135803,\n",
       " 'magnitude': 3.815000057220459,\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_scores_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elem in api_scores_data:\n",
    "#     curr_review_text = elem[\"input\"]\n",
    "#     if curr_review_text in data_df:\n",
    "#         matched_review_text = curr_review_text\n",
    "#     else:\n",
    "#         matched_key_idx = find_matched_pairs(curr_review_text, all_keys)\n",
    "#         matched_review_text = all_keys[matched_key_idx[0]]\n",
    "#     assert(\"google_api_score\" not in data_df[matched_review_text])\n",
    "#     data_df[matched_review_text][\"google_api_score\"] = elem[\"score\"]\n",
    "#     assert(\"google_api_magnitude\" not in data_df[matched_review_text])\n",
    "#     data_df[matched_review_text][\"google_api_magnitude\"] = elem[\"magnitude\"]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Kwang's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_coordinates_csv_path = \"/home/anmola/assignments_hw/sentiment-cartography/data/kwanghee_polar_coordinates.csv\"\n",
    "polar_coordinates_df = pd.read_csv(polar_coordinates_csv_path)\n",
    "\n",
    "# retain only those values in `split=test`\n",
    "polar_coordinates_df = polar_coordinates_df[polar_coordinates_df[\"split\"] == \"test\"]\n",
    "# rename columns\n",
    "rename_mapping = {'pos':\"pos_loss\", \"neg\": \"neg_loss\", \"ori\": \"base_loss\"}\n",
    "for key in rename_mapping:\n",
    "    polar_coordinates_df = polar_coordinates_df.rename(columns={key: rename_mapping[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pos_loss', 'neg_loss', 'base_loss', 'text', 'label', 'split'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polar_coordinates_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_loss</th>\n",
       "      <th>neg_loss</th>\n",
       "      <th>base_loss</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650000</th>\n",
       "      <td>3.637170</td>\n",
       "      <td>3.423840</td>\n",
       "      <td>4.001163</td>\n",
       "      <td>I got 'new' tires from them and within two wee...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650001</th>\n",
       "      <td>3.356954</td>\n",
       "      <td>3.033727</td>\n",
       "      <td>4.223260</td>\n",
       "      <td>Don't waste your time.  We had two different p...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650002</th>\n",
       "      <td>3.785630</td>\n",
       "      <td>3.524564</td>\n",
       "      <td>4.136008</td>\n",
       "      <td>All I can say is the worst! We were the only 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650003</th>\n",
       "      <td>3.353036</td>\n",
       "      <td>2.869883</td>\n",
       "      <td>3.647495</td>\n",
       "      <td>I have been to this restaurant twice and was d...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650004</th>\n",
       "      <td>3.176172</td>\n",
       "      <td>2.873306</td>\n",
       "      <td>3.618195</td>\n",
       "      <td>Food was NOT GOOD at all! My husband &amp; I ate h...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699995</th>\n",
       "      <td>3.798547</td>\n",
       "      <td>3.646442</td>\n",
       "      <td>3.949870</td>\n",
       "      <td>Just wanted to write a review to chip in with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699996</th>\n",
       "      <td>2.362278</td>\n",
       "      <td>3.172081</td>\n",
       "      <td>3.590026</td>\n",
       "      <td>Great ambience. Great drinks. Great food. I lo...</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699997</th>\n",
       "      <td>3.373806</td>\n",
       "      <td>3.480679</td>\n",
       "      <td>4.198580</td>\n",
       "      <td>I have been to the other Monks locations so I ...</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699998</th>\n",
       "      <td>3.848746</td>\n",
       "      <td>3.583591</td>\n",
       "      <td>4.204350</td>\n",
       "      <td>Don't go here.  I know you might want to try i...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699999</th>\n",
       "      <td>3.842948</td>\n",
       "      <td>3.603918</td>\n",
       "      <td>4.226319</td>\n",
       "      <td>Buffet was recently open after renovation so m...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos_loss  neg_loss  base_loss  \\\n",
       "650000  3.637170  3.423840   4.001163   \n",
       "650001  3.356954  3.033727   4.223260   \n",
       "650002  3.785630  3.524564   4.136008   \n",
       "650003  3.353036  2.869883   3.647495   \n",
       "650004  3.176172  2.873306   3.618195   \n",
       "...          ...       ...        ...   \n",
       "699995  3.798547  3.646442   3.949870   \n",
       "699996  2.362278  3.172081   3.590026   \n",
       "699997  3.373806  3.480679   4.198580   \n",
       "699998  3.848746  3.583591   4.204350   \n",
       "699999  3.842948  3.603918   4.226319   \n",
       "\n",
       "                                                     text  label split  \n",
       "650000  I got 'new' tires from them and within two wee...      0  test  \n",
       "650001  Don't waste your time.  We had two different p...      0  test  \n",
       "650002  All I can say is the worst! We were the only 2...      0  test  \n",
       "650003  I have been to this restaurant twice and was d...      0  test  \n",
       "650004  Food was NOT GOOD at all! My husband & I ate h...      0  test  \n",
       "...                                                   ...    ...   ...  \n",
       "699995  Just wanted to write a review to chip in with ...      0  test  \n",
       "699996  Great ambience. Great drinks. Great food. I lo...      4  test  \n",
       "699997  I have been to the other Monks locations so I ...      3  test  \n",
       "699998  Don't go here.  I know you might want to try i...      1  test  \n",
       "699999  Buffet was recently open after renovation so m...      0  test  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polar_coordinates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_polar(pos_loss, neg_loss, base_loss):\n",
    "    pos = np.exp(pos_loss - base_loss)\n",
    "    neg = np.exp(neg_loss- base_loss)\n",
    "    r = np.sqrt((pos ** 2 + neg ** 2))\n",
    "    theta = np.arctan(pos / neg)\n",
    "    theta_in_degrees = np.degrees(theta)\n",
    "    return r, theta, theta_in_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new columns ie polar_radius, polar_theta\n",
    "for row in polar_coordinates_df.iterrows():\n",
    "    r, theta, theta_in_degrees = _get_polar(row[1][\"pos_loss\"], row[1][\"neg_loss\"], row[1][\"base_loss\"])\n",
    "    polar_coordinates_df.at[row[0], \"polar_radius\"] = r\n",
    "    polar_coordinates_df.at[row[0], \"polar_theta\"] = theta\n",
    "    polar_coordinates_df.at[row[0], \"polar_theta_deg\"] = theta_in_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_loss</th>\n",
       "      <th>neg_loss</th>\n",
       "      <th>base_loss</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>polar_radius</th>\n",
       "      <th>polar_theta</th>\n",
       "      <th>polar_theta_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650000</th>\n",
       "      <td>3.637170</td>\n",
       "      <td>3.423840</td>\n",
       "      <td>4.001163</td>\n",
       "      <td>I got 'new' tires from them and within two wee...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.893337</td>\n",
       "      <td>0.891263</td>\n",
       "      <td>51.065621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650001</th>\n",
       "      <td>3.356954</td>\n",
       "      <td>3.033727</td>\n",
       "      <td>4.223260</td>\n",
       "      <td>Don't waste your time.  We had two different p...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.519094</td>\n",
       "      <td>0.944269</td>\n",
       "      <td>54.102620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650002</th>\n",
       "      <td>3.785630</td>\n",
       "      <td>3.524564</td>\n",
       "      <td>4.136008</td>\n",
       "      <td>All I can say is the worst! We were the only 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.889150</td>\n",
       "      <td>0.914473</td>\n",
       "      <td>52.395453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650003</th>\n",
       "      <td>3.353036</td>\n",
       "      <td>2.869883</td>\n",
       "      <td>3.647495</td>\n",
       "      <td>I have been to this restaurant twice and was d...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.875254</td>\n",
       "      <td>1.018090</td>\n",
       "      <td>58.332252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650004</th>\n",
       "      <td>3.176172</td>\n",
       "      <td>2.873306</td>\n",
       "      <td>3.618195</td>\n",
       "      <td>Food was NOT GOOD at all! My husband &amp; I ate h...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.799081</td>\n",
       "      <td>0.934568</td>\n",
       "      <td>53.546802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699995</th>\n",
       "      <td>3.798547</td>\n",
       "      <td>3.646442</td>\n",
       "      <td>3.949870</td>\n",
       "      <td>Just wanted to write a review to chip in with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>1.133103</td>\n",
       "      <td>0.861159</td>\n",
       "      <td>49.340771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699996</th>\n",
       "      <td>2.362278</td>\n",
       "      <td>3.172081</td>\n",
       "      <td>3.590026</td>\n",
       "      <td>Great ambience. Great drinks. Great food. I lo...</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>0.720631</td>\n",
       "      <td>0.418643</td>\n",
       "      <td>23.986467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699997</th>\n",
       "      <td>3.373806</td>\n",
       "      <td>3.480679</td>\n",
       "      <td>4.198580</td>\n",
       "      <td>I have been to the other Monks locations so I ...</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>0.655790</td>\n",
       "      <td>0.732063</td>\n",
       "      <td>41.944125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699998</th>\n",
       "      <td>3.848746</td>\n",
       "      <td>3.583591</td>\n",
       "      <td>4.204350</td>\n",
       "      <td>Don't go here.  I know you might want to try i...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>0.916449</td>\n",
       "      <td>52.508650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699999</th>\n",
       "      <td>3.842948</td>\n",
       "      <td>3.603918</td>\n",
       "      <td>4.226319</td>\n",
       "      <td>Buffet was recently open after renovation so m...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.867480</td>\n",
       "      <td>0.903791</td>\n",
       "      <td>51.783417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos_loss  neg_loss  base_loss  \\\n",
       "650000  3.637170  3.423840   4.001163   \n",
       "650001  3.356954  3.033727   4.223260   \n",
       "650002  3.785630  3.524564   4.136008   \n",
       "650003  3.353036  2.869883   3.647495   \n",
       "650004  3.176172  2.873306   3.618195   \n",
       "...          ...       ...        ...   \n",
       "699995  3.798547  3.646442   3.949870   \n",
       "699996  2.362278  3.172081   3.590026   \n",
       "699997  3.373806  3.480679   4.198580   \n",
       "699998  3.848746  3.583591   4.204350   \n",
       "699999  3.842948  3.603918   4.226319   \n",
       "\n",
       "                                                     text  label split  \\\n",
       "650000  I got 'new' tires from them and within two wee...      0  test   \n",
       "650001  Don't waste your time.  We had two different p...      0  test   \n",
       "650002  All I can say is the worst! We were the only 2...      0  test   \n",
       "650003  I have been to this restaurant twice and was d...      0  test   \n",
       "650004  Food was NOT GOOD at all! My husband & I ate h...      0  test   \n",
       "...                                                   ...    ...   ...   \n",
       "699995  Just wanted to write a review to chip in with ...      0  test   \n",
       "699996  Great ambience. Great drinks. Great food. I lo...      4  test   \n",
       "699997  I have been to the other Monks locations so I ...      3  test   \n",
       "699998  Don't go here.  I know you might want to try i...      1  test   \n",
       "699999  Buffet was recently open after renovation so m...      0  test   \n",
       "\n",
       "        polar_radius  polar_theta  polar_theta_deg  \n",
       "650000      0.893337     0.891263        51.065621  \n",
       "650001      0.519094     0.944269        54.102620  \n",
       "650002      0.889150     0.914473        52.395453  \n",
       "650003      0.875254     1.018090        58.332252  \n",
       "650004      0.799081     0.934568        53.546802  \n",
       "...              ...          ...              ...  \n",
       "699995      1.133103     0.861159        49.340771  \n",
       "699996      0.720631     0.418643        23.986467  \n",
       "699997      0.655790     0.732063        41.944125  \n",
       "699998      0.883173     0.916449        52.508650  \n",
       "699999      0.867480     0.903791        51.783417  \n",
       "\n",
       "[50000 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polar_coordinates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to json\n",
    "polar_coordinates_data = []\n",
    "all_cols = list(polar_coordinates_df.columns)\n",
    "for i in range(len(polar_coordinates_df)):\n",
    "    polar_coordinates_data.append({x: polar_coordinates_df.iloc[i][x] for x in all_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos_loss': 3.6371703147888175,\n",
       " 'neg_loss': 3.423840284347534,\n",
       " 'base_loss': 4.001162528991699,\n",
       " 'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!',\n",
       " 'label': 0,\n",
       " 'split': 'test',\n",
       " 'polar_radius': 0.8933369212390614,\n",
       " 'polar_theta': 0.8912632147066474,\n",
       " 'polar_theta_deg': 51.06562063795302}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polar_coordinates_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in polar_coordinates_data:\n",
    "    curr_review_text = elem[\"text\"]\n",
    "    if curr_review_text in data_df:\n",
    "        matched_review_text = curr_review_text\n",
    "    else:\n",
    "        matched_key_idx = find_matched_pairs(curr_review_text, all_keys)\n",
    "        matched_review_text = all_keys[matched_key_idx[0]]\n",
    "    cols_transfer = [\"polar_radius\", \"polar_theta\", \"polar_theta_deg\", 'pos_loss', 'neg_loss', 'base_loss']\n",
    "    for col in cols_transfer:\n",
    "        # assert(col not in data_df[matched_review_text])\n",
    "        data_df[matched_review_text][col] = elem[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Buffet was recently open after renovation so my husband and I are thinking it should be pretty good....wrong. We arrived at 8:30 am for what we thought was a Saturday brunch offering champs, nope. Price is $21.95 but on Monday - Friday is $18.95...same food so why the increase? I can go to the Fiesta in Henderson with champs and better food for $8.99 Sat/Sun but we wanted to try something we thought was going to be an upgrade....lol what a joke. \\\\n\\\\n\\\\nFood: everything I tried looked and tasted like food was leftover from the day before and reheated. \\\\n\\\\nShrimps were all water logged, the crab legs had a brownish color to them with an odd taste. \\\\n\\\\nPastries none..all offerings were donuts. They did have a fresh crepe bar, but then again looked very dry. I didn't want to stand on line for crepe paper. \\\\n\\\\nOverall I'm rating this one star because I have no choice.\",\n",
       " 'gt_label': 0,\n",
       " 'multiclass_predicted_label': 1,\n",
       " 'single_class_logit_score': 0.00017183152926799749,\n",
       " 'polar_radius': 0.8674800906028501,\n",
       " 'polar_theta': 0.9037911259264794,\n",
       " 'polar_theta_deg': 51.783417076963985,\n",
       " 'pos_loss': 3.8429481983184814,\n",
       " 'neg_loss': 3.603918075561523,\n",
       " 'base_loss': 4.226319313049316}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[matched_review_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'poss_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43melem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposs_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'poss_loss'"
     ]
    }
   ],
   "source": [
    "elem['poss_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
